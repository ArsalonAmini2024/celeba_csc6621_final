{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the final project \"sandbox\" for CSC6621 - Arsalon's Version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Imports\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "from pyunpack import Archive\n",
    "\n",
    "# Pre Processing Imports \n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from sklearn.model_selection import train_test_split # type: ignore\n",
    "import numpy as np # type: ignore\n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning Imports \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.layers import GlobalAveragePooling2D, Dense\n",
    "from keras.models import Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction successful.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the directory containing the split archive\n",
    "# archive_directory = 'data/img_celeba.7z'\n",
    "\n",
    "# Specify the first file of the split archive\n",
    "# first_part_path = os.path.join(archive_directory, 'img_celeba.7z.001')\n",
    "\n",
    "# Define the output directory for the extracted files\n",
    "# output_directory = 'data/img_celeba_extracted'\n",
    "\n",
    "# Extract the archive\n",
    "'''\n",
    "try:\n",
    "    Archive(first_part_path).extractall(output_directory)\n",
    "    print(\"Extraction successful.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to extract:\", e)\n",
    "\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "\n",
    "# Define the output directory from previous context\n",
    "output_directory = 'data/img_celeba_extracted'\n",
    "\n",
    "# Path to the directory where all images are stored\n",
    "image_directory = os.path.join(output_directory, 'img_celeba')\n",
    "\n",
    "# Define the train and test directories\n",
    "train_dir = os.path.join(output_directory, 'train')\n",
    "test_dir = os.path.join(output_directory, 'test')\n",
    "\n",
    "# Create directories for train and test datasets if they don't exist\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "# Get all file names in the output directory\n",
    "all_files = [os.path.join(image_directory, f) for f in os.listdir(image_directory) if os.path.isfile(os.path.join(image_directory, f))]\n",
    "\n",
    "# Split files into train and test sets (80-20 split here)\n",
    "train_files, test_files = train_test_split(all_files, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to copy files to new directories\n",
    "def copy_files(files, directory):\n",
    "    for file in files:\n",
    "        shutil.copy(file, directory)\n",
    "\n",
    "# Copy files to respective directories\n",
    "copy_files(train_files, train_dir)\n",
    "copy_files(test_files, test_dir)\n",
    "\n",
    "print(\"Files successfully split into training and testing directories.\")\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "train_dir = 'data/img_celeba_extracted/train'  \n",
    "test_dir = 'data/img_celeba_extracted/test'    \n",
    "\n",
    "# Load the labels dataset and ensure labels are string type\n",
    "file_path = os.path.join(os.getcwd(), 'data', 'identity_CelebA.txt')\n",
    "data = pd.read_csv(file_path, delim_whitespace=True, header=None, names=['filename', 'label'])\n",
    "data['label'] = data['label'].astype(str)  # Convert labels to string to match expected format\n",
    "\n",
    "# Check file existence for train, test, and validate subsets\n",
    "data['train_exists'] = data['filename'].apply(lambda x: os.path.exists(os.path.join(train_dir, x)))\n",
    "data['test_exists'] = data['filename'].apply(lambda x: os.path.exists(os.path.join(test_dir, x)))\n",
    "\n",
    "# Separate DataFrames for train, test based on file existence\n",
    "train_data = data[data['train_exists']]\n",
    "test_data = data[data['test_exists']]\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Train Generator - updated class_mode to 'categorical'\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=data,\n",
    "    directory=train_dir,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),  # Resizes all images to 224x224 (higher resolution)\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  \n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Validation Generator (from train data) - also 'categorical'\n",
    "validation_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=data,\n",
    "    directory=train_dir,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    ")\n",
    "\n",
    "# Test Generator - also 'categorical'\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=data,\n",
    "    directory=test_dir,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Print the shapes of input images and labels from the generator\n",
    "inputs, labels = next(train_generator)\n",
    "print('Input batch shape:', inputs.shape)\n",
    "print('Label batch shape:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model - Simple CNN\n",
    "\n",
    "num_classes = 10153 # Number of classes\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # match number of classes = 10,177 persons\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,  \n",
    "    validation_data=test_generator  # Validation data to evaluate the model\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Model v1 - Resnet Architecture\n",
    "\n",
    "# Define the base ResNet50 model\n",
    "base_model = ResNet50(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "# Add new layers on top\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)  # Adds a global spatial average pooling layer\n",
    "x = Dense(1024, activation='relu')(x)  # Add a fully-connected layer\n",
    "predictions = Dense(10177, activation='softmax')(x)  # Output layer for 10,177 classes\n",
    "\n",
    "# This is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Summary of the model\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
