{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going to try a few things on a smaller scale \n",
    "\n",
    "Smaller Scale - Use Rosie Jupyter Notebooks T4 GPUS fine \n",
    "- filter out all the images with less than 30 occurances in the label set (to ensure amply training labels)\n",
    "- Try using a batch like 1000 images and 5 classes to see if I get any performance at all (accuracy above 50%)\n",
    "- Scale up slightly to 100 classes and see if performance remains \n",
    "\n",
    "Large Scale - Use command line DGx and sbash script to run the job \n",
    "- Try to run the scaled version on DGx with Rosie and Tensorboard as well as manual logging for viewing data runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Imports\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Pre Processing Imports \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.model_selection import train_test_split \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "# Deep Learning Imports \n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "from keras.layers import GlobalAveragePooling2D, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "from keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the identity file into a DataFrame\n",
    "df = pd.read_csv('data/identity_CelebA.txt', delim_whitespace=True, header=None, names=['filename', 'label'])\n",
    "\n",
    "# Count the occurrences of each label\n",
    "label_counts = df['label'].value_counts()\n",
    "\n",
    "# Identify labels with only one occurrence\n",
    "single_count_labels = label_counts[label_counts == 1]\n",
    "print(f'Number of labels with only one occurrence: {len(single_count_labels)}')\n",
    "\n",
    "# Remove rows where labels appear less than 30x (otherwise stratification doesn't work well causing class imbalances)\n",
    "df_filtered = df[df['label'].map(label_counts) > 30]\n",
    "\n",
    "# Count the occurrences of each label\n",
    "label_counts = df_filtered['label'].value_counts()\n",
    "print(label_counts)\n",
    "\n",
    "# create train test splits based on the txt file (containing file names and labels)\n",
    "train_df, test_df = train_test_split(df_filtered, test_size=0.2, stratify=df_filtered['label'])\n",
    "\n",
    "# Convert label column to string - req for downstream datagenerators to one-hot encode them\n",
    "train_df['label'] = train_df['label'].astype(str)\n",
    "test_df['label'] = test_df['label'].astype(str)\n",
    "\n",
    "print(f\"Unique classes in training set: {train_df['label'].nunique()}\")\n",
    "print(f\"Unique classes in testing set: {test_df['label'].nunique()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the actual images to the respective folders based on the train-test split of the labels \n",
    "def copy_images(df, source_dir, target_dir):\n",
    "    os.makedirs(target_dir, exist_ok=True)  # Ensure target directory exists\n",
    "    for filename in df['filename']:\n",
    "        shutil.copy(os.path.join(source_dir, filename), os.path.join(target_dir, filename))\n",
    "\n",
    "source_directory = 'data/img_align_celeba'\n",
    "train_directory = 'data/train'\n",
    "test_directory = 'data/test'\n",
    "\n",
    "# Move train and test images\n",
    "copy_images(train_df, source_directory, train_directory)\n",
    "copy_images(test_df, source_directory, test_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Train Generator - updated class_mode to 'categorical'\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    directory=train_directory,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),  # Resizes all images to 224x224 (higher resolution)\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  \n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Test Generator - also 'categorical'\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    directory=test_directory,\n",
    "    x_col='filename',\n",
    "    y_col='label',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Print the shapes of input images and labels from the generator\n",
    "inputs, labels = next(train_generator)\n",
    "print('Input batch shape:', inputs.shape)\n",
    "print('Label batch shape:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline Model - Simple CNN\n",
    "\n",
    "num_classes = 10133 # Number of classes\n",
    "\n",
    "# Model Definition\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Flatten(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(num_classes, activation='softmax')  # match number of classes = 10,177 persons\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "tensorboard_base_model = TensorBoard(log_dir='./logs/base_model', histogram_freq=1)\n",
    "\n",
    "# Model Training\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=10,  \n",
    "    validation_data=test_generator,  # Validation data to evaluate the model\n",
    "    callbacks=[tensorboard_base_model]\n",
    ")\n",
    "\n",
    "# Model Evaluation\n",
    "test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Model v1 - Resnet Architecture - random weights\n",
    "\n",
    "# Model Definition\n",
    "base_model = ResNet50(weights=None, include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "x = base_model.output \n",
    "x = GlobalAveragePooling2D()(x)  # Adds a global spatial average pooling layer\n",
    "x = Dense(1024, activation='relu')(x)  # Add a fully-connected layer\n",
    "predictions = Dense(10133, activation='softmax')(x)  # Output layer for 10,133 classes\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # Compile the model\n",
    "\n",
    "model.summary() # Summary of the model\n",
    "\n",
    "# Create a TensorBoard callback\n",
    "tensorboard_resnet_random_weights = TensorBoard(log_dir='./logs/resnet_random_weights', histogram_freq=1)\n",
    "\n",
    "\n",
    "# Model Training\n",
    "\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=10,\n",
    "    validation_data=test_generator,\n",
    "    validation_steps=len(test_generator),\n",
    "    callbacks=[tensorboard_resnet_random_weights]\n",
    ")\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "scores = model.evaluate(test_generator, steps=len(test_generator))\n",
    "print(f\"Test Accuracy: {scores[1]*100}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
